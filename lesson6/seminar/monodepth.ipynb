{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# MonoDepth: Оценка глубины по одному изображению\n",
        "\n",
        "В этом ноутбуке мы реализуем и обучим модель для оценки глубины (depth estimation) по одному RGB изображению.\n",
        "\n",
        "**Задача:** По входному RGB изображению предсказать карту глубины (depth map), где каждый пиксель содержит расстояние до соответствующей точки сцены.\n",
        "\n",
        "**Архитектура:** Encoder-Decoder на базе MobileNetV2 (компактная и быстрая модель)\n",
        "\n",
        "**Датасет:** NYU Depth V2 (подмножество)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Импорты\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import h5py\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "pl.seed_everything(42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Загрузка датасета NYU Depth V2\n",
        "\n",
        "NYU Depth V2 — популярный датасет для задачи depth estimation. Содержит RGB изображения и соответствующие карты глубины, полученные с помощью Kinect сенсора.\n",
        "\n",
        "Используем labeled subset (~1449 изображений) для обучения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "DATA_DIR = \"../data\"\n",
        "NYU_PATH = os.path.join(DATA_DIR, \"nyu_depth_v2_labeled.mat\")\n",
        "\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "if not os.path.exists(NYU_PATH):\n",
        "    print(\"Скачиваем NYU Depth V2 датасет...\")\n",
        "    url = \"http://horatio.cs.nyu.edu/mit/silberman/nyu_depth_v2/nyu_depth_v2_labeled.mat\"\n",
        "    print(f\"URL: {url}\")\n",
        "    print(\"Это может занять несколько минут (~2.8 GB)...\")\n",
        "    !wget -q --show-progress -O {NYU_PATH} {url}\n",
        "    print(\"Готово!\")\n",
        "else:\n",
        "    print(f\"Датасет уже существует: {NYU_PATH}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Загружаем данные из .mat файла\n",
        "with h5py.File(NYU_PATH, 'r') as f:\n",
        "    print(\"Ключи в файле:\", list(f.keys()))\n",
        "    \n",
        "    # images: (3, 640, 480, N) -> транспонируем в (N, 480, 640, 3)\n",
        "    images = np.array(f['images']).transpose(0, 3, 2, 1)\n",
        "    # depths: (640, 480, N) -> транспонируем в (N, 480, 640)\n",
        "    depths = np.array(f['depths']).transpose(0, 2, 1)\n",
        "\n",
        "print(f\"Images shape: {images.shape}\")\n",
        "print(f\"Depths shape: {depths.shape}\")\n",
        "print(f\"Depth range: [{depths.min():.2f}, {depths.max():.2f}] meters\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Визуализируем несколько примеров\n",
        "fig, axes = plt.subplots(3, 2, figsize=(12, 12))\n",
        "\n",
        "for i in range(3):\n",
        "    idx = i * 100\n",
        "    \n",
        "    axes[i, 0].imshow(images[idx])\n",
        "    axes[i, 0].set_title(f'RGB Image {idx}')\n",
        "    axes[i, 0].axis('off')\n",
        "    \n",
        "    im = axes[i, 1].imshow(depths[idx], cmap='plasma')\n",
        "    axes[i, 1].set_title(f'Depth Map {idx}')\n",
        "    axes[i, 1].axis('off')\n",
        "    plt.colorbar(im, ax=axes[i, 1], label='Depth (m)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset класс\n",
        "\n",
        "Создаём PyTorch Dataset для загрузки пар (RGB, Depth).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class NYUDepthDataset(Dataset):\n",
        "    def __init__(self, images, depths, indices=None, img_size=(256, 256), augment=False):\n",
        "        self.images = images\n",
        "        self.depths = depths\n",
        "        self.indices = indices if indices is not None else list(range(len(images)))\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        \n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize(img_size),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        real_idx = self.indices[idx]\n",
        "        \n",
        "        # Загружаем RGB\n",
        "        image = Image.fromarray(self.images[real_idx].astype(np.uint8))\n",
        "        \n",
        "        # Загружаем depth\n",
        "        depth = self.depths[real_idx]\n",
        "        depth = Image.fromarray(depth.astype(np.float32), mode='F')\n",
        "        depth = depth.resize(self.img_size, Image.BILINEAR)\n",
        "        depth = np.array(depth)\n",
        "        \n",
        "        # Аугментации (горизонтальный flip)\n",
        "        if self.augment and np.random.random() > 0.5:\n",
        "            image = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "            depth = np.fliplr(depth).copy()\n",
        "        \n",
        "        # Применяем трансформации к RGB\n",
        "        image = self.transform(image)\n",
        "        \n",
        "        # Нормализуем depth (клиппим экстремальные значения)\n",
        "        depth = np.clip(depth, 0.1, 10.0)  # 0.1m - 10m range\n",
        "        depth = torch.tensor(depth, dtype=torch.float32).unsqueeze(0)\n",
        "        \n",
        "        return image, depth\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Разбиваем на train/val/test\n",
        "NUM_SAMPLES = len(images)\n",
        "indices = np.random.permutation(NUM_SAMPLES)\n",
        "\n",
        "train_size = int(0.7 * NUM_SAMPLES)\n",
        "val_size = int(0.15 * NUM_SAMPLES)\n",
        "\n",
        "train_indices = indices[:train_size].tolist()\n",
        "val_indices = indices[train_size:train_size + val_size].tolist()\n",
        "test_indices = indices[train_size + val_size:].tolist()\n",
        "\n",
        "print(f\"Train: {len(train_indices)} samples\")\n",
        "print(f\"Val: {len(val_indices)} samples\")\n",
        "print(f\"Test: {len(test_indices)} samples\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "IMG_SIZE = (256, 256)\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_dataset = NYUDepthDataset(images, depths, train_indices, IMG_SIZE, augment=True)\n",
        "val_dataset = NYUDepthDataset(images, depths, val_indices, IMG_SIZE, augment=False)\n",
        "test_dataset = NYUDepthDataset(images, depths, test_indices, IMG_SIZE, augment=False)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверяем размеры\n",
        "sample_img, sample_depth = train_dataset[0]\n",
        "print(f\"Image shape: {sample_img.shape}\")\n",
        "print(f\"Depth shape: {sample_depth.shape}\")\n",
        "print(f\"Depth range: [{sample_depth.min():.2f}, {sample_depth.max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Архитектура модели: MobileNetV2 Encoder-Decoder\n",
        "\n",
        "Используем MobileNetV2 как encoder (backbone) — компактная и эффективная модель.\n",
        "\n",
        "Decoder состоит из upsampling блоков с skip connections для восстановления пространственного разрешения.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class UpBlock(nn.Module):\n",
        "    \"\"\"Upsampling блок с skip connection\"\"\"\n",
        "    def __init__(self, in_channels, skip_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.up = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, stride=2)\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(out_channels + skip_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "    \n",
        "    def forward(self, x, skip=None):\n",
        "        x = self.up(x)\n",
        "        if skip is not None:\n",
        "            # Align sizes if needed\n",
        "            if x.shape[2:] != skip.shape[2:]:\n",
        "                x = F.interpolate(x, size=skip.shape[2:], mode='bilinear', align_corners=True)\n",
        "            x = torch.cat([x, skip], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class MonoDepthNet(nn.Module):\n",
        "    \"\"\"Encoder-Decoder модель для depth estimation на базе MobileNetV2\"\"\"\n",
        "    def __init__(self, pretrained=True):\n",
        "        super().__init__()\n",
        "        \n",
        "        # Encoder: MobileNetV2\n",
        "        mobilenet = models.mobilenet_v2(weights='IMAGENET1K_V1' if pretrained else None)\n",
        "        features = mobilenet.features\n",
        "        \n",
        "        # Разбиваем на блоки для skip connections\n",
        "        # MobileNetV2 feature maps: \n",
        "        # После block 1: 16 channels, 1/2 resolution\n",
        "        # После block 3: 24 channels, 1/4 resolution  \n",
        "        # После block 6: 32 channels, 1/8 resolution\n",
        "        # После block 13: 96 channels, 1/16 resolution\n",
        "        # После block 17: 320 channels, 1/32 resolution (но мы используем 1280 после conv)\n",
        "        \n",
        "        self.enc1 = features[:2]   # 16 ch, 1/2\n",
        "        self.enc2 = features[2:4]  # 24 ch, 1/4\n",
        "        self.enc3 = features[4:7]  # 32 ch, 1/8\n",
        "        self.enc4 = features[7:14] # 96 ch, 1/16\n",
        "        self.enc5 = features[14:]  # 1280 ch, 1/32\n",
        "        \n",
        "        # Decoder\n",
        "        self.up1 = UpBlock(1280, 96, 256)   # 1/32 -> 1/16\n",
        "        self.up2 = UpBlock(256, 32, 128)    # 1/16 -> 1/8\n",
        "        self.up3 = UpBlock(128, 24, 64)     # 1/8 -> 1/4\n",
        "        self.up4 = UpBlock(64, 16, 32)      # 1/4 -> 1/2\n",
        "        self.up5 = nn.Sequential(           # 1/2 -> 1/1\n",
        "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "        \n",
        "        # Output head (предсказываем depth)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Conv2d(16, 16, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(16, 1, kernel_size=1),\n",
        "            nn.ReLU()  # Depth всегда положительный\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Encoder\n",
        "        e1 = self.enc1(x)    # 1/2\n",
        "        e2 = self.enc2(e1)   # 1/4\n",
        "        e3 = self.enc3(e2)   # 1/8\n",
        "        e4 = self.enc4(e3)   # 1/16\n",
        "        e5 = self.enc5(e4)   # 1/32\n",
        "        \n",
        "        # Decoder with skip connections\n",
        "        d1 = self.up1(e5, e4)  # 1/16\n",
        "        d2 = self.up2(d1, e3)  # 1/8\n",
        "        d3 = self.up3(d2, e2)  # 1/4\n",
        "        d4 = self.up4(d3, e1)  # 1/2\n",
        "        d5 = self.up5(d4)      # 1/1\n",
        "        \n",
        "        # Output\n",
        "        out = self.head(d5)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Проверяем модель\n",
        "model_test = MonoDepthNet(pretrained=True)\n",
        "dummy_input = torch.randn(1, 3, 256, 256)\n",
        "output = model_test(dummy_input)\n",
        "print(f\"Input shape: {dummy_input.shape}\")\n",
        "print(f\"Output shape: {output.shape}\")\n",
        "print(f\"Parameters: {sum(p.numel() for p in model_test.parameters()):,}\")\n",
        "del model_test, dummy_input, output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Метрики для Depth Estimation\n",
        "\n",
        "Стандартные метрики для оценки качества depth estimation:\n",
        "\n",
        "**Error metrics (чем меньше, тем лучше):**\n",
        "- **Abs Rel**: $\\frac{1}{N}\\sum|d - d^*| / d^*$ — относительная абсолютная ошибка\n",
        "- **Sq Rel**: $\\frac{1}{N}\\sum(d - d^*)^2 / d^*$ — относительная квадратичная ошибка  \n",
        "- **RMSE**: $\\sqrt{\\frac{1}{N}\\sum(d - d^*)^2}$ — среднеквадратичная ошибка\n",
        "- **RMSE log**: $\\sqrt{\\frac{1}{N}\\sum(\\log d - \\log d^*)^2}$ — RMSE в логарифмическом пространстве\n",
        "\n",
        "**Accuracy metrics (чем больше, тем лучше):**\n",
        "- **$\\delta < 1.25$**: % пикселей где $\\max(d/d^*, d^*/d) < 1.25$\n",
        "- **$\\delta < 1.25^2$**: % пикселей где $\\max(d/d^*, d^*/d) < 1.25^2$\n",
        "- **$\\delta < 1.25^3$**: % пикселей где $\\max(d/d^*, d^*/d) < 1.25^3$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_depth_metrics(pred, target, min_depth=0.1, max_depth=10.0):\n",
        "    \"\"\"\n",
        "    Вычисляет стандартные метрики для depth estimation.\n",
        "    \n",
        "    Args:\n",
        "        pred: предсказанная карта глубины (B, 1, H, W)\n",
        "        target: ground truth карта глубины (B, 1, H, W)\n",
        "        min_depth: минимальная глубина для фильтрации\n",
        "        max_depth: максимальная глубина для фильтрации\n",
        "    \n",
        "    Returns:\n",
        "        dict с метриками\n",
        "    \"\"\"\n",
        "    # Создаём маску валидных пикселей\n",
        "    mask = (target > min_depth) & (target < max_depth)\n",
        "    \n",
        "    pred = pred[mask]\n",
        "    target = target[mask]\n",
        "    \n",
        "    # Клиппим предсказания\n",
        "    pred = torch.clamp(pred, min_depth, max_depth)\n",
        "    \n",
        "    # Error metrics\n",
        "    abs_rel = torch.mean(torch.abs(pred - target) / target)\n",
        "    sq_rel = torch.mean(((pred - target) ** 2) / target)\n",
        "    rmse = torch.sqrt(torch.mean((pred - target) ** 2))\n",
        "    rmse_log = torch.sqrt(torch.mean((torch.log(pred) - torch.log(target)) ** 2))\n",
        "    \n",
        "    # Accuracy metrics (delta thresholds)\n",
        "    thresh = torch.max(pred / target, target / pred)\n",
        "    delta1 = (thresh < 1.25).float().mean() * 100\n",
        "    delta2 = (thresh < 1.25 ** 2).float().mean() * 100\n",
        "    delta3 = (thresh < 1.25 ** 3).float().mean() * 100\n",
        "    \n",
        "    return {\n",
        "        'abs_rel': abs_rel,\n",
        "        'sq_rel': sq_rel,\n",
        "        'rmse': rmse,\n",
        "        'rmse_log': rmse_log,\n",
        "        'delta1': delta1,\n",
        "        'delta2': delta2,\n",
        "        'delta3': delta3\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loss функции для Depth Estimation\n",
        "\n",
        "Комбинация нескольких loss функций:\n",
        "1. **L1 Loss** — для базовой регрессии\n",
        "2. **SSIM Loss** — для сохранения структуры изображения\n",
        "3. **Gradient Loss** — для сохранения резких границ\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SSIM(nn.Module):\n",
        "    \"\"\"Structural Similarity Index\"\"\"\n",
        "    def __init__(self, window_size=11, size_average=True):\n",
        "        super().__init__()\n",
        "        self.window_size = window_size\n",
        "        self.size_average = size_average\n",
        "        self.channel = 1\n",
        "        self.window = self._create_window(window_size, self.channel)\n",
        "        \n",
        "    def _create_window(self, window_size, channel):\n",
        "        def gaussian(window_size, sigma):\n",
        "            gauss = torch.tensor([np.exp(-(x - window_size//2)**2/(2*sigma**2)) \n",
        "                                  for x in range(window_size)], dtype=torch.float32)\n",
        "            return gauss / gauss.sum()\n",
        "        \n",
        "        _1D_window = gaussian(window_size, 1.5).unsqueeze(1)\n",
        "        _2D_window = _1D_window.mm(_1D_window.t()).unsqueeze(0).unsqueeze(0)\n",
        "        window = _2D_window.expand(channel, 1, window_size, window_size).contiguous()\n",
        "        return window\n",
        "    \n",
        "    def forward(self, img1, img2):\n",
        "        channel = img1.size(1)\n",
        "        \n",
        "        if channel != self.channel or self.window.device != img1.device:\n",
        "            self.window = self._create_window(self.window_size, channel).to(img1.device)\n",
        "            self.channel = channel\n",
        "        \n",
        "        mu1 = F.conv2d(img1, self.window, padding=self.window_size//2, groups=channel)\n",
        "        mu2 = F.conv2d(img2, self.window, padding=self.window_size//2, groups=channel)\n",
        "        \n",
        "        mu1_sq = mu1.pow(2)\n",
        "        mu2_sq = mu2.pow(2)\n",
        "        mu1_mu2 = mu1 * mu2\n",
        "        \n",
        "        sigma1_sq = F.conv2d(img1*img1, self.window, padding=self.window_size//2, groups=channel) - mu1_sq\n",
        "        sigma2_sq = F.conv2d(img2*img2, self.window, padding=self.window_size//2, groups=channel) - mu2_sq\n",
        "        sigma12 = F.conv2d(img1*img2, self.window, padding=self.window_size//2, groups=channel) - mu1_mu2\n",
        "        \n",
        "        C1 = 0.01 ** 2\n",
        "        C2 = 0.03 ** 2\n",
        "        \n",
        "        ssim_map = ((2*mu1_mu2 + C1)*(2*sigma12 + C2)) / ((mu1_sq + mu2_sq + C1)*(sigma1_sq + sigma2_sq + C2))\n",
        "        \n",
        "        if self.size_average:\n",
        "            return ssim_map.mean()\n",
        "        return ssim_map.mean(1).mean(1).mean(1)\n",
        "\n",
        "\n",
        "class GradientLoss(nn.Module):\n",
        "    \"\"\"Loss на градиентах для сохранения резких границ\"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        # Градиенты по x\n",
        "        pred_dx = pred[:, :, :, :-1] - pred[:, :, :, 1:]\n",
        "        target_dx = target[:, :, :, :-1] - target[:, :, :, 1:]\n",
        "        \n",
        "        # Градиенты по y\n",
        "        pred_dy = pred[:, :, :-1, :] - pred[:, :, 1:, :]\n",
        "        target_dy = target[:, :, :-1, :] - target[:, :, 1:, :]\n",
        "        \n",
        "        loss_x = F.l1_loss(pred_dx, target_dx)\n",
        "        loss_y = F.l1_loss(pred_dy, target_dy)\n",
        "        \n",
        "        return loss_x + loss_y\n",
        "\n",
        "\n",
        "class DepthLoss(nn.Module):\n",
        "    \"\"\"Комбинированный loss для depth estimation\"\"\"\n",
        "    def __init__(self, ssim_weight=0.5, gradient_weight=0.5):\n",
        "        super().__init__()\n",
        "        self.ssim = SSIM()\n",
        "        self.gradient = GradientLoss()\n",
        "        self.ssim_weight = ssim_weight\n",
        "        self.gradient_weight = gradient_weight\n",
        "        \n",
        "    def forward(self, pred, target):\n",
        "        # L1 loss\n",
        "        l1_loss = F.l1_loss(pred, target)\n",
        "        \n",
        "        # Нормализуем для SSIM (в диапазон [0, 1])\n",
        "        pred_norm = pred / 10.0  # Предполагаем max depth = 10m\n",
        "        target_norm = target / 10.0\n",
        "        \n",
        "        # SSIM loss (1 - SSIM, так как SSIM=1 это идеальное совпадение)\n",
        "        ssim_loss = 1 - self.ssim(pred_norm, target_norm)\n",
        "        \n",
        "        # Gradient loss\n",
        "        grad_loss = self.gradient(pred, target)\n",
        "        \n",
        "        total_loss = l1_loss + self.ssim_weight * ssim_loss + self.gradient_weight * grad_loss\n",
        "        \n",
        "        return total_loss, {'l1': l1_loss, 'ssim': ssim_loss, 'gradient': grad_loss}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## PyTorch Lightning модуль\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class DepthModule(pl.LightningModule):\n",
        "    def __init__(self, learning_rate=1e-4, min_depth=0.1, max_depth=10.0):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "        \n",
        "        self.model = MonoDepthNet(pretrained=True)\n",
        "        self.criterion = DepthLoss(ssim_weight=0.5, gradient_weight=0.5)\n",
        "        self.learning_rate = learning_rate\n",
        "        self.min_depth = min_depth\n",
        "        self.max_depth = max_depth\n",
        "    \n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, depths = batch\n",
        "        pred_depths = self.forward(images)\n",
        "        \n",
        "        loss, loss_dict = self.criterion(pred_depths, depths)\n",
        "        \n",
        "        # Метрики\n",
        "        with torch.no_grad():\n",
        "            metrics = compute_depth_metrics(pred_depths, depths, self.min_depth, self.max_depth)\n",
        "        \n",
        "        self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
        "        self.log('train_abs_rel', metrics['abs_rel'], on_step=False, on_epoch=True)\n",
        "        self.log('train_rmse', metrics['rmse'], on_step=False, on_epoch=True)\n",
        "        self.log('train_delta1', metrics['delta1'], on_step=False, on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, depths = batch\n",
        "        pred_depths = self.forward(images)\n",
        "        \n",
        "        loss, loss_dict = self.criterion(pred_depths, depths)\n",
        "        metrics = compute_depth_metrics(pred_depths, depths, self.min_depth, self.max_depth)\n",
        "        \n",
        "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_abs_rel', metrics['abs_rel'], on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_rmse', metrics['rmse'], on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_delta1', metrics['delta1'], on_step=False, on_epoch=True, prog_bar=True)\n",
        "        self.log('val_delta2', metrics['delta2'], on_step=False, on_epoch=True)\n",
        "        self.log('val_delta3', metrics['delta3'], on_step=False, on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, depths = batch\n",
        "        pred_depths = self.forward(images)\n",
        "        \n",
        "        loss, _ = self.criterion(pred_depths, depths)\n",
        "        metrics = compute_depth_metrics(pred_depths, depths, self.min_depth, self.max_depth)\n",
        "        \n",
        "        self.log('test_loss', loss, on_step=False, on_epoch=True)\n",
        "        self.log('test_abs_rel', metrics['abs_rel'], on_step=False, on_epoch=True)\n",
        "        self.log('test_sq_rel', metrics['sq_rel'], on_step=False, on_epoch=True)\n",
        "        self.log('test_rmse', metrics['rmse'], on_step=False, on_epoch=True)\n",
        "        self.log('test_rmse_log', metrics['rmse_log'], on_step=False, on_epoch=True)\n",
        "        self.log('test_delta1', metrics['delta1'], on_step=False, on_epoch=True)\n",
        "        self.log('test_delta2', metrics['delta2'], on_step=False, on_epoch=True)\n",
        "        self.log('test_delta3', metrics['delta3'], on_step=False, on_epoch=True)\n",
        "        \n",
        "        return loss\n",
        "    \n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.learning_rate, weight_decay=1e-4)\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=1e-6)\n",
        "        return [optimizer], [scheduler]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Обучение модели\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = DepthModule(\n",
        "    learning_rate=1e-4,\n",
        "    min_depth=0.1,\n",
        "    max_depth=10.0\n",
        ")\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    monitor='val_abs_rel',\n",
        "    mode='min',\n",
        "    save_top_k=1,\n",
        "    filename='monodepth-{epoch:02d}-{val_abs_rel:.4f}'\n",
        ")\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_abs_rel',\n",
        "    mode='min',\n",
        "    patience=10\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    max_epochs=50,\n",
        "    accelerator='auto',\n",
        "    devices=1,\n",
        "    callbacks=[checkpoint_callback, early_stopping],\n",
        "    enable_progress_bar=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "trainer.fit(model, train_loader, val_loader)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Оценка на тестовой выборке\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_results = trainer.test(model, test_loader)\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Результаты на тестовой выборке:\")\n",
        "print(\"=\"*60)\n",
        "print(f\"\\nError metrics (чем меньше, тем лучше):\")\n",
        "print(f\"  Abs Rel:  {test_results[0]['test_abs_rel']:.4f}\")\n",
        "print(f\"  Sq Rel:   {test_results[0]['test_sq_rel']:.4f}\")\n",
        "print(f\"  RMSE:     {test_results[0]['test_rmse']:.4f} m\")\n",
        "print(f\"  RMSE log: {test_results[0]['test_rmse_log']:.4f}\")\n",
        "print(f\"\\nAccuracy metrics (чем больше, тем лучше):\")\n",
        "print(f\"  δ < 1.25:   {test_results[0]['test_delta1']:.1f}%\")\n",
        "print(f\"  δ < 1.25²:  {test_results[0]['test_delta2']:.1f}%\")\n",
        "print(f\"  δ < 1.25³:  {test_results[0]['test_delta3']:.1f}%\")\n",
        "print(\"=\"*60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Визуализация предсказаний\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
